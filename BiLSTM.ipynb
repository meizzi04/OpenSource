{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12pwhbwL2cozNAR5qaE9CiElLnziFPiEL",
      "authorship_tag": "ABX9TyP5KSN3k7lXdTL5ghSjkWxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meizzi04/OpenSource/blob/main/BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cq9_RBw3zB-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20RNN%20Sequence%20Labeling/dataset/ner_dataset.csv\", filename=\"ner_dataset.csv\")\n",
        "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")"
      ],
      "metadata": {
        "id": "fyPlJm_r4CdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "id": "iIYVJ11q5eEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('데이터프레임 행의 개수 : {}'.format(len(data)))"
      ],
      "metadata": {
        "id": "nzFvBd3n5eQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('데이터에 Null 값이 있는지 유무 : ' + str(data.isnull().values.any()))"
      ],
      "metadata": {
        "id": "ZpEdjWKW5r3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('어떤 열에 Null값이 있는지 출력')\n",
        "print('==============================')\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "S0fNafHp5urv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('sentence # 열의 중복을 제거한 값의 개수 : {}'.format(data['Sentence #'].nunique()))\n",
        "print('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))\n",
        "print('Tag 열의 중복을 제거한 값의 개수 : {}'.format(data.Tag.nunique()))"
      ],
      "metadata": {
        "id": "1RWjvIqs5wQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tag 열의 각각의 값의 개수 카운트')\n",
        "print('================================')\n",
        "print(data.groupby('Tag').size().reset_index(name='count'))"
      ],
      "metadata": {
        "id": "H_JTpbn25ynA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.fillna(method=\"ffill\")  #NULL 값을 가진 행의 바로 앞의 행의 값으로 NULL 값을 채우는 함수\n",
        "print(data.tail())"
      ],
      "metadata": {
        "id": "F5pdSnAD50sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('데이터에 Null 값이 있는지 유무 : ' + str(data.isnull().values.any()))"
      ],
      "metadata": {
        "id": "udqVtjb952or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Word'] = data['Word'].str.lower()\n",
        "print('Word 열의 중복을 제거한 값의 개수 : {}'.format(data.Word.nunique()))"
      ],
      "metadata": {
        "id": "gDfNVGms53uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[:5])"
      ],
      "metadata": {
        "id": "yhkk0Gl25456"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func = lambda temp: [(w, t) for w, t in zip(temp[\"Word\"].values.tolist(), temp[\"Tag\"].values.tolist())]\n",
        "tagged_sentences=[t for t in data.groupby(\"Sentence #\").apply(func)]\n",
        "print(\"전체 샘플 개수: {}\".format(len(tagged_sentences)))"
      ],
      "metadata": {
        "id": "2zIjc6j556Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_sentences[0]) # 첫번째 샘플 출력"
      ],
      "metadata": {
        "id": "SDsfdGEC58LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, ner_tags = [], [] \n",
        "for tagged_sentence in tagged_sentences: # 47,959개의 문장 샘플을 1개씩 불러온다.\n",
        "\n",
        "    # 각 샘플에서 단어들은 sentence에 개체명 태깅 정보들은 tag_info에 저장.\n",
        "    sentence, tag_info = zip(*tagged_sentence) \n",
        "    sentences.append(list(sentence)) # 각 샘플에서 단어 정보만 저장한다.\n",
        "    ner_tags.append(list(tag_info)) # 각 샘플에서 개체명 태깅 정보만 저장한다."
      ],
      "metadata": {
        "id": "vH78YpnD5-_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])\n",
        "print(ner_tags[0])"
      ],
      "metadata": {
        "id": "ut5Lg8fY6A9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[98])\n",
        "print(ner_tags[98])"
      ],
      "metadata": {
        "id": "80HHUUnS6CzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
        "print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5EXD_zPv6El0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 단어를 사용하며 인덱스 1에는 단어 'OOV'를 할당.\n",
        "src_tokenizer = Tokenizer(oov_token='OOV')\n",
        "# 태깅 정보들은 내부적으로 대문자를 유지한 채 저장\n",
        "tar_tokenizer = Tokenizer(lower=False)\n",
        "\n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "tar_tokenizer.fit_on_texts(ner_tags)"
      ],
      "metadata": {
        "id": "OVqy-Wn96G7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(src_tokenizer.word_index) + 1\n",
        "tag_size = len(tar_tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
        "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))"
      ],
      "metadata": {
        "id": "xPlIFumb6I7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(src_tokenizer.word_index['OOV']))"
      ],
      "metadata": {
        "id": "mRiz-mxj6KaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_data = tar_tokenizer.texts_to_sequences(ner_tags)"
      ],
      "metadata": {
        "id": "z1spSqPy6NPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data[0])\n",
        "print(y_data[0])"
      ],
      "metadata": {
        "id": "6QN_ZIiy6QDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = src_tokenizer.word_index\n",
        "index_to_word = src_tokenizer.index_word\n",
        "ner_to_index = tar_tokenizer.word_index\n",
        "index_to_ner = tar_tokenizer.index_word\n",
        "index_to_ner[0] = 'PAD'\n",
        "\n",
        "print(index_to_ner)"
      ],
      "metadata": {
        "id": "rd4SHzt56TNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = []\n",
        "for index in X_data[0] : # 첫번째 샘플 안의 인덱스들에 대해서\n",
        "    decoded.append(index_to_word[index]) # 다시 단어로 변환\n",
        "\n",
        "print('기존의 문장 : {}'.format(sentences[0]))\n",
        "print('디코딩 문장 : {}'.format(decoded))"
      ],
      "metadata": {
        "id": "bmMnKZta6Uxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 70\n",
        "X_data = pad_sequences(X_data, padding='post', maxlen=max_len)\n",
        "y_data = pad_sequences(y_data, padding='post', maxlen=max_len)"
      ],
      "metadata": {
        "id": "c-kiVFbQ6Wbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train_int, y_test_int = train_test_split(X_data, y_data, test_size=.2, random_state=777)"
      ],
      "metadata": {
        "id": "z1rvBbcy6X34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train_int, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test_int, num_classes=tag_size)"
      ],
      "metadata": {
        "id": "Xh7ySEr66ZEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블(정수 인코딩)의 크기 : {}'.format(y_train_int.shape))\n",
        "print('훈련 샘플 레이블(원-핫 인코딩)의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블(정수 인코딩)의 크기 : {}'.format(y_test_int.shape))\n",
        "print('테스트 샘플 레이블(원-핫 인코딩)의 크기 : {}'.format(y_test.shape))"
      ],
      "metadata": {
        "id": "xc0Dd9L_6aan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=6, validation_split=0.1)"
      ],
      "metadata": {
        "id": "K8lP79SG6cRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "y_predicted = model.predict(np.array([X_test[i]])) # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = np.argmax(y_predicted, axis=-1) # 확률 벡터를 정수 인코딩으로 변경함.\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경함.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
      ],
      "metadata": {
        "id": "2En33Dcr6ePk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\n",
        "predicted = ['O'] * len(labels) \n",
        "print('예측값 :',predicted)"
      ],
      "metadata": {
        "id": "GU82etmsDFaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hit = 0 # 정답 개수\n",
        "for tag, pred in zip(labels, predicted):\n",
        "    if tag == pred:\n",
        "        hit +=1 # 정답인 경우에만 +1\n",
        "accuracy = hit/len(labels) # 정답 개수를 총 개수로 나눈다.\n",
        "print(\"정확도: {:.1%}\".format(accuracy))"
      ],
      "metadata": {
        "id": "Nzuh1NhBDIQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "PkXFpoRUDKzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "print(classification_report([labels], [predicted]))"
      ],
      "metadata": {
        "id": "9A6WCwwoDM80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','B-MISC','I-MISC','I-MISC','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O']\n",
        "predicted = ['B-PER', 'I-PER', 'O', 'O', 'B-MISC', 'O','O','O','O','O','O','O','O','O','O','B-PER','I-PER','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O','O']\n",
        "\n",
        "print(classification_report([labels], [predicted]))"
      ],
      "metadata": {
        "id": "WeySEhZUDQan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "def sequences_to_tag(sequences):\n",
        "    result = []\n",
        "    # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
        "    for sequence in sequences:\n",
        "        word_sequence = []\n",
        "        # 시퀀스로부터 확률 벡터 또는 원-핫 벡터를 하나씩 꺼낸다.\n",
        "        for pred in sequence:\n",
        "            # 정수로 변환. 예를 들어 pred가 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
        "            pred_index = np.argmax(pred)            \n",
        "            # index_to_ner을 사용하여 정수를 태깅 정보로 변환. 'PAD'는 'O'로 변경.\n",
        "            word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
        "        result.append(word_sequence)\n",
        "    return result\n",
        "\n",
        "y_predicted = model.predict([X_test])\n",
        "pred_tags = sequences_to_tag(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "O-k-Y-ORDST3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CRF 층  추가 - 성능 향상"
      ],
      "metadata": {
        "id": "5ux6LpzODaEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-crf"
      ],
      "metadata": {
        "id": "4o_O-ocbDVKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Bidirectional, TimeDistributed, Embedding, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras_crf import CRFModel\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 64\n",
        "dropout_ratio = 0.3\n",
        "\n",
        "sequence_input = Input(shape=(max_len,),dtype=tf.int32, name='sequence_input')\n",
        "\n",
        "model_embedding = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=max_len)(sequence_input)\n",
        "\n",
        "model_bilstm = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(model_embedding)\n",
        "\n",
        "model_dropout = TimeDistributed(Dropout(dropout_ratio))(model_bilstm)\n",
        "\n",
        "model_dense = TimeDistributed(Dense(tag_size, activation='relu'))(model_dropout)\n",
        "\n",
        "base = Model(inputs=sequence_input, outputs=model_dense)\n",
        "model = CRFModel(base, tag_size)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')"
      ],
      "metadata": {
        "id": "rLt2CF8EDohW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])"
      ],
      "metadata": {
        "id": "FFlbCWutD0Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('bilstm_crf/cp.ckpt')\n",
        "\n",
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "y_predicted = model.predict(np.array([X_test[i]]))[0] # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred])) "
      ],
      "metadata": {
        "id": "Cuz3O8sID2lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict(X_test)[0]"
      ],
      "metadata": {
        "id": "U_cC25X0IKWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_predicted[:2])"
      ],
      "metadata": {
        "id": "ToJIYAdnId-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequences_to_tag_for_crf(sequences): \n",
        "    result = []\n",
        "    # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
        "    for sequence in sequences: \n",
        "        word_sequence = []\n",
        "        # 시퀀스로부터 예측 정수 레이블을 하나씩 꺼낸다.\n",
        "        for pred_index in sequence:\n",
        "            # index_to_ner을 사용하여 정수를 태깅 정보로 변환. 'PAD'는 'O'로 변경.\n",
        "            word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
        "        result.append(word_sequence)\n",
        "    return result\n",
        "\n",
        "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "sa6d4ezIIj7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}